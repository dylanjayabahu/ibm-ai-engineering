Transformers for sequential data 
    e.g. time series, text, audio
    lets u capture long range dependencies for this type of data    
        much better at this than rnns or lstm 
        does so with self attention mechanisms
        as such state of the art approach for sequential data
    
    Sequential data = order is important + dependences of elements on previous elements
        e.g. natural language text; meaning of word depends on previous words
        or time series data; each data is influenced by passed values 


# multi headed attention layer in keras
    # note that keras comes with a MultiHeadAttention, this is just for illustrative purposes

class MultiHeadSelfAttention(Layer):
    def __init(self, embed_dim, num_heads=8):
        super(MultiHeadedAttention, self).__init__()

        self.embed_dim = embed_dim #previously called this d_model in other code from lesson 1 
        self.num_heads = num_heads 
        self.projection_dim = embed_dim // num_heads

        self.query_dense = Dense(embed_dim)
        self.key_dense = Dense(embed_dim)
        self.value_dense = Dense(embed_dim)

        self.combine_heads = Dense(embed_dim)

    def attention(self, query, key, value): # we saw this prevore in prev lesson
        # q = self.query_dense(inputs)
        # k = self.key_dense(inputs)
        # v = self.value_dense(inputs)
        ^ this would already be done as we pass into the attention method 
        q = query 
        k = key 
        v = value

        attention_weights = tf.matmul(q, k, transpose_b=True) # dotproduct of the matrices
        attention_weights /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)), axis=-1 
            # dot products get big fast, so we scale down the scores by sqrt(self.projection_dim)
            # in this case, scale down by sqrt(64)
            # otherwise softmax becomes very peaky, almost one hot encoded
        attention_weights = tf.nn.softmax(attention_weights)

        output = tf.matmul(attention_weights, v)

        return output, attention_weights
    
    def split_heads(self, x, batch_size): 
        # instead of a single 512 dim attention, run 8 smaller attentions in parallel => 8 attention heads, each with dim 64
        # 64 is the self.projection_dim, the per-head embedding dim 

        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) 
            # e.g. reshape x from (32, 60, 512) to be (32, 60, 8, 64)

        return tf.transpose(x, perm=[0, 2, 1, 3]) 
            # then transform from (32, 60, 8, 64) to (32, 8, 60, 64)
            # it is now of the form (batch, num_heads, num_tokens, per_head_embedding_dim)
            # this way attention can be effficiently applied for each head 

     
    
    def call(self, inputs)
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)
        key = self.key_dense(inputs)
        value = self.value_dense(inputs)

        query = self.split_heads(query, batch_size)  # (batch, num_heads, seq_len, projection_dim)
        key = self.split_heads(key, batch_size)
        value = self.split_heads(value, batch_size)


        attention, attention_weights = self.attention(query, key, value)
        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) # (batch, seq_len, num_heads, projection_dim)


        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) # (batch, seq_len, embed_dim)

        output = self.combine_heads(concat_attention) 
            # combine_heads is a dense layer that mixes the concatenated heads into the final embed_dim again
            # bsaically recombining those 8 heads back into 1

        return output




# now that we have multilayer attention, we can make a transfomre block class (attention + feed forward netowrk, including norm/dropout etc)

class TransformerBlock(Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):
        #ff_dim is the hidden layer dims in the feed forward part 
        
        super(TransformerBlock, self).__init__()

        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = tf.keras.Sequential([
            Dense(ff_dim, activation='relu'),
            Dense(embed_dim),
        ])

        self.dropout1 = Dropout(dropout_rate)
        self.dropout2 = Dropout(dropout_rate)

        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)


    def call(self, inputs, training=False):
        self_attention_output = self.att(inputs)
        self_attention_output = self.dropout1(self_attention_output, training=training)
        self_attention_output = self.layernorm1(inputs + self_attention_output)

        ffn_output = self.ffn(self_attention_output)
        ffn_output = self.dropout1(ffn_output, training=training)
        ffn_output = self.layernorm2(self_attention_output + ffn_output)

        return ffn_output

# example usage 
embed_dim = 128 
num_heads = 8
ff_dim = 512 
seq_length = 100
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
inputs = tf.random.uniform((1, sequence_length, embed_dim))
outputs = transformer_block(inputs)
print(outputs.shape) # same shape; 1, 100, 128. 
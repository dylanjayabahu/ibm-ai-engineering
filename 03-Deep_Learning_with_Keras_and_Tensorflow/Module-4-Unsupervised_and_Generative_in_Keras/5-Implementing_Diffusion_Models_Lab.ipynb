{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67cb517e-70fc-4aaf-94d1-2e925a3c037e",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f01355-3831-46aa-a0eb-811544237cf0",
   "metadata": {},
   "source": [
    "# **Lab: Implementing Diffusion Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13be64-9822-4504-b699-f633eadc2c97",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea5d17-c11f-4aa8-890d-354ff61b1601",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to implement, train, and evaluate diffusion models using Keras. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da0e54-eb38-4118-a011-653ed3f11a4c",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will: \n",
    "- Acquire practical understanding of diffusion model architectures, data processing, model training, and performance evaluation \n",
    "- Implement, train, and evaluate diffusion models using Keras \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994de13-5928-4e48-a316-22f1e8b37e77",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ecbf4-35ce-4cb8-8599-9436bd512905",
   "metadata": {},
   "source": [
    "### Prerequisites \n",
    "\n",
    "- Basic understanding of Python and Keras \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88136ec7-4595-4acf-8010-f39ff6fb6686",
   "metadata": {},
   "source": [
    "### Steps \n",
    "\n",
    "#### Step 1: Preprocess data \n",
    "\n",
    "Prepare the MNIST data set for training by normalizing the pixel values and reshaping the images to have a single color channel. Normalization helps in faster convergence during training, and reshaping is required because the input layer of your diffusion model expects a three-dimensional tensor. \n",
    "\n",
    "**1. Load and preprocess the MNIST data set:**\n",
    "\n",
    "- Use Keras to load the MNIST data set. \n",
    "- Normalize the image pixel values to the range [0, 1]. \n",
    "\n",
    "**2. Reshape the Data:**\n",
    "- Expand the dimensions of the images to match the input shape required by the model (28x28x1). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfee63d-d913-4d22-9e6e-aa25aeba00b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !pip install tensorflow-cpu==2.16.2\n",
    "\n",
    "# import os\n",
    "# # Suppress oneDNN optimizations and lower TensorFlow logging level\n",
    "# os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39cad9-1dc3-4fe7-8893-5eb4873db83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44330a-4d5b-4078-84ab-a10dc7f7a108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d0999a-f4ca-4e6b-b313-b39b855412c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data set  \n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values  \n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Expand dimensions to match the input shape (28, 28, 1)  \n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Add noise to the data\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "# Clip the values to be within the range [0, 1]\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe84f35-06cf-459f-881c-933f17445542",
   "metadata": {},
   "source": [
    "#### Step 2: Build the diffusion model \n",
    "\n",
    "Build a simple diffusion model with an encoder that compresses the input image into a latent representation and a decoder that reconstructs the image from this representation. The model is compiled with the Adam optimizer and binary cross-entropy loss. \n",
    "\n",
    "**1. Define the encoder:**\n",
    "- Create an input layer with the shape (28, 28, 1). \n",
    "- Add two Conv2D layers with increasing filter sizes and ReLU activation. \n",
    "\n",
    "**2. Define the bottleneck:**\n",
    "- Add a flattened layer followed by a dense layer with ReLU activation. \n",
    "\n",
    "**3. Define the decoder:**\n",
    "- Add a Dense layer to expand the bottleneck representation.  \n",
    "- Reshape the output to match the original image dimensions.  \n",
    "- Add two Conv2DTranspose layers with decreasing filter sizes and ReLU activation.\n",
    "  \n",
    "**4. Compile the model:**\n",
    "- Use the Adam optimizer and binary cross-entropy loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2542121-c5f0-4a07-ab9a-dbefbb1d34af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 17:02:06.544205: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-08-25 17:02:06.544259: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-08-25 17:02:06.544265: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-08-25 17:02:06.544457: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-25 17:02:06.544475: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,696</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,630,720</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,624</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">145</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m1,605,696\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │     \u001b[38;5;34m1,630,720\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape (\u001b[38;5;33mReshape\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m4,624\u001b[0m │\n",
       "│ (\u001b[38;5;33mConv2DTranspose\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │           \u001b[38;5;34m145\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,255,233</span> (12.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,255,233\u001b[0m (12.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,255,233</span> (12.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,255,233\u001b[0m (12.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the diffusion model architecture with reduced complexity\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)  # Reduced filters\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)  # Reduced size\n",
    "x = Dense(28*28*32, activation='relu')(x)  # Reduced size\n",
    "x = Reshape((28, 28, 32))(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
    "x = Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)  # Reduced filters\n",
    "output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "diffusion_model = Model(input_layer, output_layer)\n",
    "\n",
    "# Compile the model with mixed precision and a different loss function\n",
    "diffusion_model.compile(optimizer='adam', loss='mean_squared_error')  # Using MSE for regression tasks\n",
    "\n",
    "# Summary of the optimized model\n",
    "diffusion_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f4e680-2555-4199-8848-0b96e5ddc5c5",
   "metadata": {},
   "source": [
    "#### Step 3: Add noise to the data \n",
    "\n",
    "Add random noise to the data set to simulate the diffusion process: \n",
    "- Add Gaussian noise to the training and test data sets.  \n",
    "- Clip the values to ensure they remain within the valid range [0, 1].  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e7b38e-4e9f-4815-962d-9a021d67e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache and prefetch the data using TensorFlow data pipelines for faster loading\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_noisy, x_train))\n",
    "train_dataset = train_dataset.cache().batch(64).prefetch(tf.data.AUTOTUNE)  # Reduced batch size\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test_noisy, x_test))\n",
    "val_dataset = val_dataset.cache().batch(64).prefetch(tf.data.AUTOTUNE)  # Reduced batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de619a-0b3e-476b-b236-cd0a4df0d60d",
   "metadata": {},
   "source": [
    "#### Step 4: Train the diffusion model \n",
    "\n",
    "Train the diffusion model to denoise the MINIST images. Use the noisy images as input and the original images as the target, learning to reverse the noise addition process. \n",
    "- Use the ‘fit’ method to train the model on the noisy training data. \n",
    "- Set the number of epochs to 50 and the batch size to 128. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f7b629-a1b1-43ec-a3b4-20020f275c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 17:03:31.276633: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 39ms/step - loss: 0.0637 - val_loss: 0.0221\n",
      "Epoch 2/3\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 34ms/step - loss: 0.0190 - val_loss: 0.0164\n",
      "Epoch 3/3\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 34ms/step - loss: 0.0152 - val_loss: 0.0146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x106f27a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement early stopping based on validation loss\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping and smaller batch size\n",
    "diffusion_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=3,\n",
    "    shuffle=True,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76e5e3-2472-45aa-bc70-8b01081e60b4",
   "metadata": {},
   "source": [
    "#### Step 5: Evaluate the diffusion model \n",
    "\n",
    "Evaluate the performance of the trained diffusion model by predicting the denoised images and visualizing the results. Comparing the original, noisy, and denoised images will help you understand how well the model has learned to remove noise from the images. \n",
    "\n",
    "**1. Reconstruct images:**\n",
    "- Use the diffusion model to predict the denoised test images.  \n",
    "- Compare the original, noisy, and denoised images. \n",
    "\n",
    "**2. Visualize the results:**\n",
    "- Plot a few examples of original, noisy, and denoised images side by side. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd1a3e-e453-48bc-bd03-4bcec3730f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict the denoised images\n",
    "denoised_images = diffusion_model.predict(x_test_noisy)\n",
    "\n",
    "# Visualize the results\n",
    "n = 10  # Number of digits to display\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display noisy\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display denoised\n",
    "    ax = plt.subplot(3, n, i + 1 + 2*n)\n",
    "    plt.imshow(denoised_images[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bea54-d184-4d25-93d2-c24e45f6d0a7",
   "metadata": {},
   "source": [
    "#### Step 6: Fine-tune the diffusion model \n",
    "\n",
    "Fine-tune the diffusion model by unfreezing some layers and retraining the model to improve its performance. \n",
    "\n",
    "\n",
    "**1. Freeze the model layers:**\n",
    "- Freeze all the layers of the encoder.\n",
    "\n",
    "**2. Check the Status:** \n",
    "- Checking the trainable status of each layer.\n",
    "\n",
    "**3. Unfreeze the model layers:** \n",
    "- Unfreeze the last few layers of the model to allow them to be retrained. \n",
    "\n",
    "**4. Compile and train the model:** \n",
    "- Recompile the model. \n",
    "- Train the model again for an additional 10 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c91dd-da47-4010-8690-ae8904619e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the layers \n",
    "for layer in diffusion_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935ed64-212b-4612-b606-cc91482c14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check trainable status of each layer\n",
    "for i, layer in enumerate(diffusion_model.layers):\n",
    "    print(f\"Layer {i}: {layer.name} — Trainable: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c72b7-bb60-449a-ac24-b3fb98a89901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the top layers of the model\n",
    "for layer in diffusion_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model again\n",
    "diffusion_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model again\n",
    "diffusion_model.fit(x_train_noisy, x_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test_noisy, x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f137b4-7b20-44d5-bb56-58c83a37243b",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "### Exercise 1: Modify the noise factor  \n",
    "\n",
    "#### Objective: \n",
    "- Change the noise factor and see how it affects the model’s ability to denoise images.\n",
    "#### Instructions:  \n",
    "1. Change the noise factor to 0.3.  \n",
    "2. Add noise to the training and test data sets with the new noise factor.  \n",
    "3. Retrain the model with the new noisy data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb2842-5d8d-464d-8ea0-ef633576023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6640b89-e79c-4e03-8efc-e4343a65e7b3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Modify the noise factor to 0.3  \n",
    "noise_factor = 0.3  \n",
    "   \n",
    "# Add noise to the data with the new noise factor  \n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)  \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)  \n",
    "  \n",
    "# Clip the values to be within the range [0, 1]  \n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)  \n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)  \n",
    "   \n",
    "# Retrain the model  \n",
    "diffusion_model.fit(x_train_noisy, x_train,    \n",
    "                    epochs=50,    \n",
    "                    batch_size=128,    \n",
    "                    shuffle=True,    \n",
    "                    validation_data=(x_test_noisy, x_test))  \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140e83e-f99d-48d7-80c5-e45cc559e2b1",
   "metadata": {},
   "source": [
    "### Exercise 2 - Add more layers to the model  \n",
    "\n",
    "#### Objective: \n",
    "- Experiment with adding more layers to the model to see how it affects performance.\n",
    "\n",
    "#### Instructions:\n",
    "1. Add an additional Conv2D layer with 128 filters in the encoder.  \n",
    "2. Add an additional Conv2DTranspose layer with 128 filters in the decoder.  \n",
    "3. Rebuild, compile, and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b55e01-f28c-4845-8918-1d77c121e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64b5b5-e439-4fc1-a9f6-18b696fa6704",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Define the modified diffusion model architecture with additional layers\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x) # Additional layer\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(28*28*64, activation='relu')(x)\n",
    "x = Reshape((28, 28, 64))(x)\n",
    "x = Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x) # Additional layer\n",
    "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "output_layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "diffusion_model = Model(input_layer, output_layer)\n",
    "\n",
    "# Compile the model  \n",
    "diffusion_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "   \n",
    "\n",
    "# Summary of the model  \n",
    "diffusion_model.summary()\n",
    "\n",
    "# Train the model  \n",
    "diffusion_model.fit(x_train_noisy, x_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_test_noisy, x_test))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe68b7-e302-4103-832c-185301948d58",
   "metadata": {},
   "source": [
    "### Exercise 3: Visualize the effect of noise  \n",
    "\n",
    "#### Objective: \n",
    "- Compare the impact of different noise levels on the denoising performance of the model.\n",
    "\n",
    "#### Instructions:  \n",
    "1. Add noise with different factors (e.g., 0.1, 0.5, 0.7) to the test data.  \n",
    "2. Use the model to predict the denoised images for each noise level.  \n",
    "3. Visualize the original, noisy, and denoised images side by side for each noise level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae6679-aa0e-4a0f-8e10-d8a90ee29550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14ead7-3807-46fb-9f8e-b59252a8e500",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "\n",
    "# Function to add noise and predict denoised images\n",
    "def add_noise_and_predict(noise_factor):\n",
    "    x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "    x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "    denoised_images = diffusion_model.predict(x_test_noisy)\n",
    "    return x_test_noisy, denoised_images\n",
    "\n",
    "# Noise levels to test\n",
    "noise_levels = [0.1, 0.5, 0.7]\n",
    "   \n",
    "# Visualize the results\n",
    "n = 5  # Number of digits to display\n",
    "plt.figure(figsize=(20, 12))\n",
    "for idx, noise_factor in enumerate(noise_levels):\n",
    "    x_test_noisy, denoised_images = add_noise_and_predict(noise_factor)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(3 * len(noise_levels), n, i + 1 + idx * 3 * n)\n",
    "        plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_title(f'Original (Noise: {noise_factor})')\n",
    "          \n",
    "        # Display noisy\n",
    "        ax = plt.subplot(3 * len(noise_levels), n, i + 1 + n + idx * 3 * n)\n",
    "        plt.imshow(x_test_noisy[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "\n",
    "\n",
    "        # Display denoised\n",
    "        ax = plt.subplot(3 * len(noise_levels), n, i + 1 + 2 * n + idx * 3 * n)\n",
    "        plt.imshow(denoised_images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b74ed3-d0f0-465a-b590-043e0f366160",
   "metadata": {},
   "source": [
    "### Summary  \n",
    "\n",
    "By completing these exercises, students will:  \n",
    "1. Understand the impact of different noise factors on the model’s denoising capabilities.\n",
    "2. Learn how adding more layers to the model affects its performance.\n",
    "3. Visualize how different levels of noise affect the denoising results of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dceec4b-6767-44af-af06-573cbc3f91a2",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "Congratulations! You have gained practical experience in implementing diffusion models using Keras. You learned how to preprocess data, construct a basic diffusion model architecture, add noise to the data set, train the model, and evaluate its performance. Additionally, you explored fine-tuning techniques to enhance the model’s performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05451c3-1e5e-44cc-8489-026d0da53323",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "prev_pub_hash": "27706737bf8740a3a1a40766707767fe01f58bbbeb5b1152a891f5d820b8a925"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

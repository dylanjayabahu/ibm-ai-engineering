Unsupervised Learning: find patterns without labels or predefined outcomes 
    no target variable
    understand underlying structure

Categories of Unsupervsed Learning:
    clustering
        grouping data points into clusters 
        similar data points in same cluster 
        e.g. k means and hierarchical clustering 

    association 
        find relationships between variables in large datasets 
        used in market basket analysis 
        e.g. A-priori and Eclat algorithms 

    dimensionality reduction 
        reduce number of random variables w/o much loss of info 
        optain principle variables
        PCA, UMAP, t-SNE


Autoencoders:
    type of nn used for dim reduction or feature learning 

    encoder:
        compresses input into a latent space representaiton 
    decoder:
        reconstructs input from latent space representation 

    
    latent space is the reduced dim vector 

    goal is to match reconstructed output to input with minimal diff


# autoencoder in keras 
input_layer = Input(shape=(784,))
encoded = Dense(64, activation='relu')(input_layer)
decoded = Dense(784, activation='sigmoid')(encoded)
autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.summary()

# can train this on mnist or something else 
autoencoder.fit(X_train, X_train, ...)





Generative Adversarial Networks (GANs)
    
    system of two networks:
        generator network generates new data instances 
        discriminator network evaluates authenticity of generated data 

        generator tries to fool discriminator, discriminator tries to suss out fake data 

        the adversarial/competing process makes the generator make better and better data 
    
# implement GAN in keras 

generator_input_dim = 100

def build_generator():
    model = tf.Keras.Sequential()
    modeadd(Dense(128, input_dim=generator_input_dim))
    model.add(LeakyReLU(alpha=0.01))
    model.add(Dense,784, activation='tanh')
    return model 
    

def build_discriminator():
    model = tf.Keras.Sequential()
    model.add(Dense(128, input_shape=(784,)))
    model.add(LeakyReLU(alpha=0.01)) # leaky relu is used specifically in GANS because it stabilizes training
    model.add(Dense(1, activation='sigmoid'))
    return model

discriminator = build_discriminator()
discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

generator = build_generator()

# make GAN by combining generator and discriminator
discriminator.trainable=False # only applied to the gan; we never recompile discriminator itself 
gan_input = Input(shape=(generator_input_dim, ))
generated_image = generator(gan_input)
gan_output = discriminator(generated_image)
gan = Model(gan_input, gan_output)

gan.compile(optimizer='adam', loss='binary_crossentropy')


# need custom training loop so u can alternate back and forth between generator and discriminator 
def train_gan(gan, generator, discriminator, x_train, epochs=400, batch_size=128):
    for epoch in range(epochs):

        #make noise as input for generator
        noise = np.random.normal(0, 1, (batch)size, generator_input_dim)
        generated_images = generator.predict(noise)


        #get random set of actual images 
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        real_images = x_train[idx]

        # labels for real/fake images 
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))


        # get training loss on discriminator on real/fake images
        d_loss_real = discriminator.train_on_batch(real_images, real_labels)
        d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)

        # compute average loss
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)


        # now train generator on new noise throug GAN model 
            # remember discriminator weights are frozen 

        noise = np.random.normal(0, 1, (batch_size, generator_input_dim))
        g_loss = gan.train_on_batch(noise, real_labels, return_dict=True)
            # since we are passing in real labels for the fake images we are making, 
            # we are teaching the generator to make images that the discriminator would predict as real

        print(f"Epoch {epoch}: d_loss={d_loss[0]}, g_loss={g_loss['loss']})
Deep Q Network (DQNs)
    extension of q learnings
    use deep q networks to approximate the q-value function 

    use experience replay and target networks 

Q-value approximation 
    use a nn to apprximate Q(s, a)

Experience replay 
    store agnet experiences in a replay buffer 
    random samples from the buffer update the network
    ^ breaks correlation between consequtive samples; improves training stability 

Target network 
    an additional netowrk 
    generates target q-values 
    updated less frequently than primary network 
    hence stable target values 
    prevent oscillations 

    without this, the q network is chassing values that itself gets updated as the network gets updated 
        like a dog chasing its tail kinda 
        because of how the bellman equation works: Q(s, a) += alpha * [ (r + gamma * Max(Q(s', a'))) - (Q(s, a)) ]
    so, we make a second network with the same architecture, but update the weights less frequently
        this way its not constantly chasing itself; it lets the dog catch up to the tail before moving the tail 

Implementing DQN with keras 
    initialize environment and parameters 
        define env with something like gym 
        set hyperparameters for training
        initialize replay buffer
    build dqn and target network 
        same architecture 
        target networks are updated less frequently
    implement experience replay 
        store agent experiences in replay buffer
        sample minibatches from the buffer to update q network 
    train q network 
        iteratively update q-values with bellman equation 
        update using gradients
            based on loss between predicted and target q values 
    evaluate 
        let agent iteract in env with policy 
        measure cumulative reward

    

# code implementatin of dqn 
import gym
import numpy as np
from collections import deque

# Initialize the environment
env = gym.make('CartPole-v1')

# Set hyperparameters
alpha = 0.001  # Learning rate
gamma = 0.99   # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_min = 0.01
epsilon_decay = 0.995
episodes = 1000
batch_size = 64
memory_size = 2000

# Initialize replay buffer
memory = deque(maxlen=memory_size) # (hehe data structs and algos :)

# Get state and action sizes
state_size = env.observation_space.shape[0]
action_size = env.action_space.n


# build networks 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

def build_q_network(state_size, action_size):
    model = Sequential()
    model.add(Dense(24, input_dim=state_size, activation='relu'))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(action_size, activation='linear'))
    model.compile(loss='mse', optimizer=Adam(learning_rate=alpha))
    return model

# Build the Q-Network and target network
q_network = build_q_network(state_size, action_size)
target_network = build_q_network(state_size, action_size)
target_network.set_weights(q_network.get_weights()) # have target and q network start as same 


# implement experience replay 

def remember(state, action, reward, next_state, done):
    memory.append((state, action, reward, next_state, done))

def replay(batch_size):
    minibatch = np.random.choice(len(memory), batch_size, replace=False)
    for index in minibatch:
        state, action, reward, next_state, done = memory[index]
        target = q_network.predict(state)
        if done:
            target[0][action] = reward
        else:
            t = target_network.predict(next_state)
            target[0][action] = reward + gamma * np.amax(t)
        q_network.fit(state, target, epochs=1, verbose=0)

# train loop 

# Initialize replay buffer
memory = deque(maxlen=memory_size)

# Get state and action sizes
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

# Main training loop
for episode in range(episodes):
    state, info = env.reset()
    state = np.reshape(state, [1, state_size])
    total_reward = 0

    for time in range(500):
        if np.random.rand() <= epsilon:
            action = np.random.choice(action_size)
        else:
            q_values = q_network.predict(state, verbose=0)
        
        next_state, reward, done, truncated, info = env.step(action)
        next_state = np.reshape(next_state, [1, state_size])
        total_reward += reward

        if done:
            reward = -10
            remember(state, action, reward, next_state, done)
            break

        state = next_state

    if len(memory) > batch_size:
        replay(batch_size)

    if epsilon > epsilon_min:
        epsilon *= epsilon_decay

    # Update the target network weights every 10 episodes
    if episode % 10 == 0:
        target_network.set_weights(q_network.get_weights())

    print(f"Episode: {episode+1}/{episodes}, Score: {total_reward}")

# evaluate agent
for episode in range(10):
    state, info = env.reset()
    state = np.reshape(state, [1, state_size])
    total_reward = 0

    for time in range(500):
        env.render() # render for visualization
        q_values = q_network.predict(state, verbose=0)
        action = np.argmax(q_values[0])
        next_state, reward, done, trunc, _ = env.step(action)
        next_state = np.reshape(next_state, [1, state_size])
        total_reward += reward
        state = next_state
        if done:
            print(f"Episode: {episode+1}, Score: {total_reward}")
            break

env.close()